{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45230523",
   "metadata": {},
   "source": [
    "### What are prompts?\n",
    "Prompts are input instructions given to a model to generate specific outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6500d835",
   "metadata": {},
   "source": [
    "\n",
    "### Static Prompt vs Dynamic Prompt in LangChain\n",
    "\n",
    "1. **Static Prompt**:  \n",
    "    A static prompt is a predefined, unchanging string that is passed to the language model. It remains constant regardless of the context or input data. Static prompts are useful for simple and repetitive tasks where the input does not vary significantly.\n",
    "\n",
    "2. **Dynamic Prompt**:  \n",
    "    A dynamic prompt is generated programmatically based on the context or input data. It allows for greater flexibility and customization, making it suitable for tasks where the input or requirements change dynamically.\n",
    "    In LangChain, dynamic prompts are often created using templates and variables to adapt to different inputs.\n",
    "\n",
    "By leveraging static and dynamic prompts effectively, you can tailor the behavior of language models to meet specific application needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10daf236",
   "metadata": {},
   "outputs": [],
   "source": [
    "## static example\n",
    "prompt = \"Summarize the plot of the movie Inception in one sentence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530d92aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Hungary is Budapest.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model = ChatOpenAI()\n",
    "result = model.invoke(\"What is the capital of Hungary?\")\n",
    "\n",
    "print(result.content)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1909bb01",
   "metadata": {},
   "source": [
    "\"\"\"This is not rightway to send prompt to the model, instead user should give prompt as input to the invoke method, we will fix it in the next file\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf84e64",
   "metadata": {},
   "source": [
    "\n",
    "## ❌ Problem with Static Prompting\n",
    "\n",
    "- The The type of prompt used in ui were static, which is **not a good practice**.\n",
    "- For example:\n",
    "  - If the user is trying to ask for the **summary of one paper first**, then another, the user has to **write the prompt every time**.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Issues with User-Controlled Prompting\n",
    "\n",
    "- The problem with the above approach is:\n",
    "  - The **user has full control** over the prompt.\n",
    "  - Prompts are **sensitive** to variations or mistakes.\n",
    "  \n",
    "- For example:\n",
    "  - If we are building a **tool for research paper summarization**, and the user types the **wrong paper name**, then:\n",
    "    - The LLM is going to **hallucinate**\n",
    "    - It will give a **false or undesirable outcome**\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Proposed Solution\n",
    "\n",
    "To fix this issue:\n",
    "\n",
    "- We will create a **prompt template**.\n",
    "- We will take **some structured inputs** from the user.\n",
    "- We will use these inputs every time to ensure:\n",
    "  - **Consistent prompts**\n",
    "  - **Reliable and accurate results**\n",
    "\n",
    "SEE THE EXAMPLE BELOW FOR BETTER UNDERSTANDING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de464e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please summarize the research paper titled \"{paper_input}\" with the following\\nspecifications:\\nExplanation Style: {style_input}\\nExplanation Length: (length_input}\\n1. Mathematical Details:\\n- Include relevant mathematical equations if present in the paper.\\nExplain the mathematical concepts using simple, intuitive code snippets\\nwhere applicable.\\n2. Analogies:\\n- Use relatable analogies to simplify complex ideas.\\nIf certain information is not available in the paper, respond with: \"Insufficient\\ninformation available\" instead of guessing.\\nEnsure the summary is clear, accurate, and aligned with the provided style and length '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dynamic example\n",
    "\"\"\"Please summarize the research paper titled \"{paper_input}\" with the following\n",
    "specifications:\n",
    "Explanation Style: {style_input}\n",
    "Explanation Length: (length_input}\n",
    "1. Mathematical Details:\n",
    "- Include relevant mathematical equations if present in the paper.\n",
    "Explain the mathematical concepts using simple, intuitive code snippets\n",
    "where applicable.\n",
    "2. Analogies:\n",
    "- Use relatable analogies to simplify complex ideas.\n",
    "If certain information is not available in the paper, respond with: \"Insufficient\n",
    "information available\" instead of guessing.\n",
    "Ensure the summary is clear, accurate, and aligned with the provided style and length \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b3c48af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets add dynamic prompts to the ui we made in the previous file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe6ceb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npaper_input = st.selectbox(\\n    \"Select a research paper\", [\"Attention Is All You Need\", \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"GPT-3: Language Models are Few-Shot Learners\", \"ResNet: Deep Residual Learning for Image Recognition\"])\\n\\n\\nstyle_input = st.selectbox(\\n    \"Select explanation style\", [\"Technical\", \"Layman\", \"Analogy-based\", \"Step-by-step\", \"Code snippets\",\"mathematical\"])\\n\\nlength_input = st.selectbox(\\n    \"Select explanation length\", [\"Brief\", \"Detailed\", \"Concise\", \"In-depth\", \"Summary\"])\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "paper_input = st.selectbox(\n",
    "    \"Select a research paper\", [\"Attention Is All You Need\", \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"GPT-3: Language Models are Few-Shot Learners\", \"ResNet: Deep Residual Learning for Image Recognition\"])\n",
    "\n",
    "\n",
    "style_input = st.selectbox(\n",
    "    \"Select explanation style\", [\"Technical\", \"Layman\", \"Analogy-based\", \"Step-by-step\", \"Code snippets\",\"mathematical\"])\n",
    "\n",
    "length_input = st.selectbox(\n",
    "    \"Select explanation length\", [\"Brief\", \"Detailed\", \"Concise\", \"In-depth\", \"Summary\"])\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc928d46",
   "metadata": {},
   "source": [
    "\n",
    "A PromptTemplate in LangChain is a structured way to create prompts dynamically\n",
    "inserting variables into a predefined template. Instead of hardcoding prompts,\n",
    "Prompt Template allows you to define placeholders that can be filled in at runtime with\n",
    "different inputs.\n",
    "This makes it reusable, flexible, and easy to manage, especially when working with dynamic\n",
    "user inputs or automated workflows.\n",
    "Why use PromptTemplate over f strings?\n",
    "\n",
    "1. Default validation (validate_template = True)\n",
    "2. Reusability (as a json file) ## lets create prompt generateor file\n",
    "3. LangChain Ecosystem (chains in langchain)\n",
    "lets see chains in langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12ad7e3",
   "metadata": {},
   "source": [
    "### after all of this lets create a simple terminal based bot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8a077",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai)",
   "language": "python",
   "name": "genai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
